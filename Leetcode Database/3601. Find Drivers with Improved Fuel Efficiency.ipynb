{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0126536c-422e-4cf6-9710-025230bad319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, FloatType\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# ðŸ§ Drivers Data\n",
    "drivers_data = [\n",
    "    (1, \"Alice Johnson\"),\n",
    "    (2, \"Bob Smith\"),\n",
    "    (3, \"Carol Davis\"),\n",
    "    (4, \"David Wilson\"),\n",
    "    (5, \"Emma Brown\")\n",
    "]\n",
    "\n",
    "drivers_schema = StructType([\n",
    "    StructField(\"driver_id\", IntegerType(), True),\n",
    "    StructField(\"driver_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "drivers = spark.createDataFrame(drivers_data, schema=drivers_schema)\n",
    "\n",
    "# ðŸš— Trips Data\n",
    "trips_data = [\n",
    "    (1, 1, \"2023-02-15\", 120.5, 10.2),\n",
    "    (2, 1, \"2023-03-20\", 200.0, 16.5),\n",
    "    (3, 1, \"2023-08-10\", 150.0, 11.0),\n",
    "    (4, 1, \"2023-09-25\", 180.0, 12.5),\n",
    "    (5, 2, \"2023-01-10\", 100.0, 9.0),\n",
    "    (6, 2, \"2023-04-15\", 250.0, 22.0),\n",
    "    (7, 2, \"2023-10-05\", 200.0, 15.0),\n",
    "    (8, 3, \"2023-03-12\", 80.0, 8.5),\n",
    "    (9, 3, \"2023-05-18\", 90.0, 9.2),\n",
    "    (10, 4, \"2023-07-22\", 160.0, 12.8),\n",
    "    (11, 4, \"2023-11-30\", 140.0, 11.0),\n",
    "    (12, 5, \"2023-02-28\", 110.0, 11.5)\n",
    "]\n",
    "\n",
    "trips_schema = StructType([\n",
    "    StructField(\"trip_id\", IntegerType(), True),\n",
    "    StructField(\"driver_id\", IntegerType(), True),\n",
    "    StructField(\"trip_date\", StringType(), True),  # You can cast this to DateType if needed\n",
    "    StructField(\"distance_km\", FloatType(), True),\n",
    "    StructField(\"fuel_consumed\", FloatType(), True)\n",
    "])\n",
    "\n",
    "trips = spark.createDataFrame(trips_data, schema=trips_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25fa9220-de8d-4f85-995b-e90a20485dd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drivers.show()\n",
    "trips.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d109bd2-935a-48be-b145-038bf74f7caf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c1e7ed4-dc06-4a15-b203-f04468c38df3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "first_half = trips.filter((month(col('trip_date'))>=1) & (month(col('trip_date'))<=6))\\\n",
    "    .groupBy('driver_id').agg((sum(col('distance_km')/col('fuel_consumed'))/count('*')).alias('first_half_avg'))\n",
    "second_half = trips.filter((month(col('trip_date'))>=7) & (month(col('trip_date'))<=12))\\\n",
    "    .groupBy('driver_id').agg((sum(col('distance_km')/col('fuel_consumed'))/count('*')).alias('second_half_avg'))\n",
    "\n",
    "first_half.join(second_half, on='driver_id', how='inner')\\\n",
    "    .filter('first_half_avg<second_half_avg')\\\n",
    "    .withColumn('efficiency_improvement',col('second_half_avg')-col('first_half_avg'))\\\n",
    "    .join(drivers, on='driver_id',how='inner')\\\n",
    "    .select('driver_id','driver_name', round('first_half_avg',2).alias('first_half_avg'),round('second_half_avg',2).alias('second_half_avg'), round('efficiency_improvement',2).alias('efficiency_improvement'))\\\n",
    "    .orderBy(desc('efficiency_improvement'),'driver_name')\\\n",
    "    .show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3601. Find Drivers with Improved Fuel Efficiency",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
